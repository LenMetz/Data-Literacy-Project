{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffcd26d2",
   "metadata": {},
   "source": [
    "### REQUIRED PACKAGES ###\n",
    "- numpy\n",
    "- pandas\n",
    "- matplotlib\n",
    "- scipy\n",
    "- pytrends\n",
    "`pip install pytrends`\n",
    "- twarc \n",
    "`pip install --upgrade twarc`\n",
    "\n",
    "*Note: twarc requires credential of a twitter developer account. Follow setup [here](https://twarc-project.readthedocs.io/en/latest/twarc2_en_us/):*\n",
    "\n",
    "*For ease of reproduction data is provided with code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7fcdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats, interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "from pytrends.request import TrendReq\n",
    "import datetime\n",
    "from datetime import date\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib\n",
    "import matplotlib.cbook as cbook\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43064a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pulls twitter data and saves to .csv file ###\n",
    "def pull_twitter_data(keyword, granularity):\n",
    "    if not os.path.exists(\"tweet_data\"):\n",
    "        os.makedirs(\"tweet_data\")\n",
    "    os.system(\"twarc2 counts {} --csv --granularity {} > tweet_data/{}.csv\".format(keyword, granularity, keyword.replace(\" \", \"_\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1010fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get twitter data from saved files ###\n",
    "def load_twitter_data(keyword, granularity):\n",
    "    dft = pd.read_csv('tweet_data/{}.csv'.format(keyword.replace(\" \", \"_\")), parse_dates=['start', 'end'])\n",
    "    dft = dft.sort_values('start')\n",
    "    twitter_data = dft[\"{}_count\".format(granularity)].to_numpy()\n",
    "    return twitter_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2babe78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pulls google data and saves to file ###\n",
    "def pull_google_data(keyword):\n",
    "    if not os.path.exists(\"google_data\"):\n",
    "        os.makedirs(\"google_data\")\n",
    "    today = date.today()\n",
    "    pytrends = TrendReq(hl='en-US', tz=0)\n",
    "    pytrends.build_payload([keyword], timeframe = 'now 7-d', geo='')\n",
    "    dfg = pytrends.interest_over_time()\n",
    "    dfg.to_csv(\"google_data/{}.csv\".format(keyword.replace(\" \", \"_\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5366862",
   "metadata": {},
   "outputs": [],
   "source": [
    "### loads google data from saved file and returns as numpy array\n",
    "def load_google_data(keyword):\n",
    "    dfg = pd.read_csv(\"google_data/{}.csv\".format(keyword.replace(\" \", \"_\")))\n",
    "    return dfg[keyword].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef0d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preset keywords based on twitter trends\n",
    "keywords_ger = [\"pistorius\", \"verteidigungsminister\", \"parität\", \"lambrecht\", \"bargeld\", \"ryerson\", \"thelastofus\", \"davos\", \"wochenstart\", \"kompetenz\", \n",
    "            \"leopard2\", \"rblfcb\", \"panzer\", \"ramstein\", \"schnee\", \"scholz\", \"putin\", \"livche\", \"russland\", \"veganer\",\n",
    "           \"bvbfca\", \"wochenstart\", \"schönensonntag\", \"übertragungsfehler\", \"annewill\", \"ukraine\", \"panzer\", \"polen\", \"arsmun\", \"impfung\"]\n",
    "granularity = \"hour\"\n",
    "creation_date_ger = datetime.datetime(2023,1,23,18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb7d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pull data for keywords\n",
    "### !!! overwrites current data files\n",
    "'''for keyword in keywords:\n",
    "    pull_twitter_data(keyword, granularity)\n",
    "    pull_google_data(keyword)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d07b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculates pearson r for given data x and y\n",
    "def pearson(keywords):\n",
    "    p_coeffs = []\n",
    "    for keyword in keywords:\n",
    "        google_data = load_google_data(keyword)\n",
    "        twitter_data = load_twitter_data(keyword, granularity=\"hour\")\n",
    "        twitter_data = (twitter_data-np.amin(twitter_data))/(np.amax(twitter_data)-np.amin(twitter_data))\n",
    "        google_data = (google_data-np.amin(google_data))/(np.amax(google_data)-np.amin(google_data))\n",
    "        length = min(len(twitter_data), len(google_data))\n",
    "        p_coeffs.append(scipy.stats.pearsonr(twitter_data[:length], google_data[:length]))\n",
    "    return np.asarray(p_coeffs)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb58ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### plots keyword usage over time in google and twitter\n",
    "def plot_data(keywords, creation_date):\n",
    "    if not os.path.exists(\"images\"):\n",
    "        os.makedirs(\"images\")\n",
    "    google_length = len(load_google_data(keywords[0]))\n",
    "    twitter_length = len(load_twitter_data(keywords[0], granularity=\"hour\"))\n",
    "    datetimes = []\n",
    "    #t = pd.date_range(start=creation_date, end=\"{}-{}-{}-{}\".format(creation_date.year, creation_date.month, creation_date.day-7, creation_date.hour), periods=7*24*60)\n",
    "    d=7\n",
    "    for hh in range(0, max(google_length,twitter_length), 1):\n",
    "        h = (hh+creation_date.hour)%24\n",
    "        if h==0: d-=1\n",
    "        #for m in range(60):\n",
    "        datetimes.append(datetime.datetime(2023,1,creation_date.day-d,h))\n",
    "    rs = pearson(keywords)\n",
    "    for ii, keyword in enumerate(keywords):\n",
    "        google_data = load_google_data(keyword)\n",
    "        twitter_data = load_twitter_data(keyword, granularity=\"hour\")\n",
    "        google_length = len(google_data)\n",
    "        twitter_length = len(twitter_data)\n",
    "        twitter_data = (twitter_data-np.amin(twitter_data))/(np.amax(twitter_data)-np.amin(twitter_data))\n",
    "        google_data = (google_data-np.amin(google_data))/(np.amax(google_data)-np.amin(google_data))\n",
    "        fig, ax = plt.subplots(figsize=(2, 2))\n",
    "        #ax.xaxis.set_minor_locator(mdates.HourLocator(byhour=(0,12)))\n",
    "        #ax.xaxis.set_minor_locator(hours)\n",
    "        #ax.set_xlim(datetimes[0], datetimes[-1])\n",
    "        matplotlib.use(\"pgf\")\n",
    "        matplotlib.rcParams.update({\n",
    "            \"pgf.texsystem\": \"pdflatex\",\n",
    "            'font.family': 'serif',\n",
    "            'text.usetex': True,\n",
    "            'pgf.rcfonts': False,\n",
    "        })\n",
    "        ax.plot(datetimes[:google_length], google_data, label=\"google\", color = \"orangered\", linewidth = \"1.0\")\n",
    "        ax.plot(datetimes[:twitter_length], twitter_data, label=\"twitter\", color = \"deepskyblue\", linewidth = \"1.0\")\n",
    "        ax.xaxis.set_major_locator(mdates.DayLocator())\n",
    "        ax.xaxis.set_minor_locator(mdates.HourLocator(byhour=(12)))\n",
    "        ax.tick_params(axis='both', which='major', labelsize=5)\n",
    "        ax.set_xlabel(\"dates\",fontsize = 8)\n",
    "        ax.set_ylabel(\"keyword per h (scaled)\",fontsize = 8)\n",
    "        ax.set_xlim(xmin=datetimes[0], xmax=datetimes[-1])\n",
    "        ax.set_ylim(ymin=0)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.set_title('\"'+keyword+'\"'+\"   PCC: {}\".format(round(rs[ii], 3)),fontsize = 8)\n",
    "        ax.legend(fontsize = 8)\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%d.%m'))\n",
    "        fig.autofmt_xdate()\n",
    "        plt.savefig('images/timeseries_{}.pgf'.format(keyword), dpi=500,  bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159230a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### plotting linear correlation between two set of data points and returning m\n",
    "def correlation_lg(word,x,y):\n",
    "    #matplotlib.pyplot.rcdefaults\n",
    "    #matplotlib.use(\"agg\")\n",
    "    length = min(len(x), len(y))\n",
    "    x = x[:length]\n",
    "    y = y[:length]\n",
    "    slope, intercept, r, p, std_err = stats.linregress(x,y)\n",
    "    def myfunc(t):\n",
    "        return slope*t+intercept\n",
    "    mymodel = list(map(myfunc, x))\n",
    "    fig, ax = plt.subplots(figsize=(2,2))\n",
    "    ax.scatter(x[:length],y[:length], color=\"black\", s=2)\n",
    "    ax.set_title('\"'+word+'\"'+\"   m: {}\".format(slope), fontsize = 8)\n",
    "    ax.plot(x, mymodel, color=\"lime\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b2c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot twitter trends \n",
    "print(creation_date_ger)\n",
    "plot_data(keywords_ger, creation_date_ger)\n",
    "for word in keywords_ger:\n",
    "    correlation_lg(word, load_google_data(word), load_twitter_data(word, granularity=\"hour\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate and print PCC\n",
    "rs = pearson(keywords_ger)\n",
    "t_maxes = []\n",
    "print(\"Keywords from trends:\\nMean: \", np.mean(rs),\"  std:\", np.std(rs))\n",
    "for ii, word in enumerate(keywords_ger):\n",
    "    twitter_data = load_twitter_data(word, granularity=\"hour\")\n",
    "    t_maxes.append(np.max(twitter_data))\n",
    "    print(word, rs[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad129cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### testing for random word out of most common nouns\n",
    "dfw = pd.read_csv(\"nounlist.csv\")\n",
    "words = dfw[\"ATM\"].to_list()\n",
    "keywords = []\n",
    "\n",
    "for i in range(30):\n",
    "    word = random.choice(words)\n",
    "    keywords.append(word)\n",
    "    pull_twitter_data(word, granularity=\"hour\")\n",
    "    pull_google_data(word)\n",
    "    \n",
    "creation_date = datetime.datetime.now() \n",
    "rs = pearson(keywords)\n",
    "print(\"Random keywords:\\nMean: \", np.mean(rs),\"  std:\", np.std(rs))\n",
    "for ii, word in enumerate(keywords):\n",
    "    print(word, rs[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164bddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot random words\n",
    "plot_data(keywords, creation_date)\n",
    "for ii, word in enumerate(keywords):\n",
    "    correlation_lg(word, load_google_data(word), load_twitter_data(word, granularity=\"hour\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
